#!/usr/bin/env python3
"""
Memory 2.0 - åŒå±‚å‹ç¼©æ–¹æ¡ˆ
1. å¯¹è¯å†å² â†’ æ¸²æŸ“ PNG
2. DeepSeek-OCR â†’ æå–æ–‡å­— (ä½œä¸º Vision Tokens)
3. å¯é€‰: ç”¨ LLM ç”Ÿæˆæ‘˜è¦

æ ¸å¿ƒä»·å€¼: é•¿å¯¹è¯å†å²å­˜å‚¨ä¸ºå›¾ç‰‡ï¼Œéœ€è¦æ—¶ç”¨ OCR è¯»å–
"""

import os
import subprocess
from pathlib import Path
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont

MODEL_PATH = os.path.expanduser("~/.lmstudio/models/mlx-community/DeepSeek-OCR-2-bf16")
PYTHON = "/opt/homebrew/bin/python3.13"
MEMORY_DIR = Path(os.path.expanduser("~/clawd/memory/vision"))
MEMORY_DIR.mkdir(parents=True, exist_ok=True)


def render_text_to_image(text: str, output_path: Path, 
                          width: int = 1200, font_size: int = 14) -> dict:
    """å°†æ–‡æœ¬æ¸²æŸ“ä¸ºé«˜å¯†åº¦ PNG"""
    
    # åŠ è½½å­—ä½“
    try:
        font = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", font_size)
    except:
        font = ImageFont.load_default()
    
    # è®¡ç®—å¸ƒå±€
    lines = []
    max_chars = width // (font_size // 2 + 2)
    
    for para in text.split('\n'):
        while len(para) > max_chars:
            lines.append(para[:max_chars])
            para = para[max_chars:]
        lines.append(para)
    
    line_height = font_size + 6
    height = len(lines) * line_height + 40
    
    # æ¸²æŸ“
    img = Image.new('RGB', (width, height), 'white')
    draw = ImageDraw.Draw(img)
    
    y = 20
    for line in lines:
        draw.text((20, y), line, fill='black', font=font)
        y += line_height
    
    img.save(output_path, 'PNG', optimize=True)
    
    return {
        "path": str(output_path),
        "width": width,
        "height": height,
        "lines": len(lines),
        "chars": len(text)
    }


def ocr_extract(image_path: str) -> str:
    """ç”¨ DeepSeek-OCR æå–å›¾ç‰‡ä¸­çš„æ–‡å­—"""
    
    cmd = [
        PYTHON, "-m", "mlx_vlm.generate",
        "--model", MODEL_PATH,
        "--image", image_path,
        "--max-tokens", "1000",
        "--prompt", "Read and output all the text in this image exactly as written:"
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
    output = result.stdout + result.stderr
    
    # æå–æ–‡å­—
    lines = output.split('\n')
    text_lines = []
    capture = False
    
    for line in lines:
        if "exactly as written:" in line:
            capture = True
            continue
        if capture:
            if "==========" in line or "Prompt:" in line:
                break
            if line.strip():
                text_lines.append(line)
    
    return '\n'.join(text_lines)


def archive_conversation(text: str, label: str = None) -> dict:
    """
    å½’æ¡£å¯¹è¯åˆ° Vision Memory
    
    è¿”å›:
    - image_path: å›¾ç‰‡è·¯å¾„
    - token_estimate: åŸå§‹ token ä¼°ç®—
    - vision_tokens: ä½œä¸ºå›¾ç‰‡åªéœ€ ~256 vision tokens
    """
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    label = label or timestamp
    image_path = MEMORY_DIR / f"{label}.png"
    
    # æ¸²æŸ“
    info = render_text_to_image(text, image_path)
    
    # ä¼°ç®— tokens
    orig_tokens = len(text) // 2  # ç²—ç•¥ä¼°ç®—
    vision_tokens = 256  # Gemini/Claude å¤„ç†å›¾ç‰‡çº¦ç”¨ 256-400 tokens
    
    return {
        "label": label,
        "image_path": str(image_path),
        "original_chars": len(text),
        "original_tokens_est": orig_tokens,
        "vision_tokens": vision_tokens,
        "compression_ratio": round(orig_tokens / vision_tokens, 1),
        "recoverable": True  # å¯ä»¥ç”¨ OCR æ¢å¤åŸæ–‡
    }


def recall_memory(image_path: str) -> str:
    """ä» Vision Memory æ¢å¤å¯¹è¯"""
    return ocr_extract(image_path)


# ===== æ¼”ç¤º =====
if __name__ == "__main__":
    
    # æ¨¡æ‹Ÿé•¿å¯¹è¯
    conversation = """
=== å¯¹è¯è®°å½• 2026-02-01 ===

ç”¨æˆ·: å¸®æˆ‘é…ç½® OpenClaw çš„æ¨¡å‹
åŠ©æ‰‹: å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ é…ç½®å¤šä¸ª AI æ¨¡å‹ providerã€‚

ç”¨æˆ·: åŠ ä¸Š NVIDIA çš„æ¨¡å‹
åŠ©æ‰‹: å·²é…ç½® NVIDIA providerï¼ŒåŒ…å«:
- Kimi K2.5 (Moonshot)
- Llama 3.3 70B (Meta)
- DeepSeek R1 Distill 70B
- Llama 3.1 405B
- Phi-4 Multimodal

ç”¨æˆ·: DeepSeek å®˜æ–¹ API ä¹Ÿè¦
åŠ©æ‰‹: å·²æ·»åŠ  DeepSeek provider:
- deepseek-chat (V3)
- deepseek-reasoner (R1)

ç”¨æˆ·: Google AI Studio
åŠ©æ‰‹: é…ç½®å®Œæˆï¼Œ8 ä¸ª Gemini æ¨¡å‹:
- Gemini 3 Pro/Flash Preview
- Gemini 2.5 Pro/Flash
- Gemini 2.0 Flash/Lite

ç”¨æˆ·: æ€»å…±å¤šå°‘æ¨¡å‹ï¼Ÿ
åŠ©æ‰‹: 42 ä¸ªäº‘ç«¯æ¨¡å‹ + æœ¬åœ° DeepSeek-OCR-2

=== å…³é”®é…ç½® ===
ä¸»åŠ›: Claude Opus 4.5
Fallback: Gemini â†’ GPT-5.2 â†’ Kimi K2.5
æœ¬åœ°: DeepSeek-OCR-2 (Memory å‹ç¼©)

=== å¾…åŠ ===
- Memory 2.0 å‹ç¼©æ–¹æ¡ˆ
- AI å®¢æœå»¶è¿Ÿæµ‹è¯•
"""
    
    print("=" * 60)
    print("ğŸ§  Memory 2.0 - Vision Archive Demo")
    print("=" * 60)
    
    # å½’æ¡£
    print("\nğŸ“¦ å½’æ¡£å¯¹è¯...")
    result = archive_conversation(conversation, "session_20260201")
    
    print(f"   åŸå§‹: ~{result['original_tokens_est']} tokens")
    print(f"   Vision: ~{result['vision_tokens']} tokens")
    print(f"   å‹ç¼©æ¯”: {result['compression_ratio']}x")
    print(f"   å›¾ç‰‡: {result['image_path']}")
    
    # æ¢å¤
    print("\nğŸ” ä» Vision Memory æ¢å¤...")
    recovered = recall_memory(result['image_path'])
    print(f"   æ¢å¤äº† {len(recovered)} å­—ç¬¦")
    print("\nğŸ“– æ¢å¤çš„å†…å®¹ (å‰ 500 å­—ç¬¦):")
    print("-" * 40)
    print(recovered[:500])
    print("-" * 40)
